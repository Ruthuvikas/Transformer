{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwIj1Q6UTXksGHpBQ8Uh+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruthuvikas/Transformer/blob/main/Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization of a tensor from fp32 to fp16"
      ],
      "metadata": {
        "id": "BbJK5fSZSCq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def quantize_example():\n",
        "    # Create a sample tensor in fp32\n",
        "    original_tensor = torch.randn(3, 4, dtype=torch.float32)\n",
        "\n",
        "    print(\"Original Tensor (FP32):\")\n",
        "    print(\"Dtype:\", original_tensor.dtype)\n",
        "    print(\"Values:\", original_tensor)\n",
        "    print(\"Memory usage (bytes):\", original_tensor.element_size() * original_tensor.nelement())\n",
        "\n",
        "    # Convert to fp16\n",
        "    quantized_tensor = original_tensor.half()\n",
        "\n",
        "    print(\"\\nQuantized Tensor (FP16):\")\n",
        "    print(\"Dtype:\", quantized_tensor.dtype)\n",
        "    print(\"Values:\", quantized_tensor)\n",
        "    print(\"Memory usage (bytes):\", quantized_tensor.element_size() * quantized_tensor.nelement())\n",
        "\n",
        "    # Demonstrate precision loss\n",
        "    print(\"\\nPrecision Comparison:\")\n",
        "    print(\"Max absolute difference:\",\n",
        "          torch.max(torch.abs(original_tensor - quantized_tensor.float())).item())\n",
        "\n",
        "# Run the demonstration\n",
        "quantize_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-wjVYXiQBa8",
        "outputId": "52c75f1b-d3f2-4a19-da9f-dc65049c33df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor (FP32):\n",
            "Dtype: torch.float32\n",
            "Values: tensor([[ 0.3416,  1.3372,  0.1772,  0.8209],\n",
            "        [-1.0045,  0.2167, -0.8730, -0.4546],\n",
            "        [-0.4399, -1.0325, -0.2025, -2.4493]])\n",
            "Memory usage (bytes): 48\n",
            "\n",
            "Quantized Tensor (FP16):\n",
            "Dtype: torch.float16\n",
            "Values: tensor([[ 0.3416,  1.3369,  0.1772,  0.8208],\n",
            "        [-1.0049,  0.2167, -0.8730, -0.4546],\n",
            "        [-0.4399, -1.0322, -0.2025, -2.4492]], dtype=torch.float16)\n",
            "Memory usage (bytes): 24\n",
            "\n",
            "Precision Comparison:\n",
            "Max absolute difference: 0.0003980398178100586\n"
          ]
        }
      ]
    }
  ]
}